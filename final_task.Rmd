---
title: "Analisis"
author: "Reinier Mujica"
date: "29 de diciembre de 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Descripción del dataset
El dataset seleccionado es [Health Insurance and Hours Worked By Wives](https://vincentarelbundock.github.io/Rdatasets/doc/Ecdat/HI.html), el cual contiene información acerca de la covertura medica y la cantidad de horas semanales trabajadas por las mujeres casadas en Estados Unidos. La información que contiene el dataset fue recopilada en el año 1993, contiene 22272 observaciones con 13 características cada una. El dataset no contiene valores NA. En este análisis de los datos trataremos de predecir la variable **whrswk** que es la cantidad de horas trabajadas a la semana por las esposas.


### Limpiamos el workspace de R
```{r}
rm(list=ls())
setwd("d:/MADM/Analisis/statics_final_task/")

```

### Cargamos los datos
```{r}
datos <- read.csv2('HI.csv', dec=".", sep = ",")

# eliminamos la primera columna que es solo un contador de filas
datos["X"] = NULL

# eliminamos posibles valores NA dentro del dataset
datos = na.omit(datos)
attach(datos)
```

### Resumen del dataset
A continuación se muestra un resumen del dataset usando la función **summary** de R.
```{r}
library(knitr)
data_summary = summary(datos)
kable(data_summary)
```

### Dividimos el dataset en TRAIN y TEST
Ahora dividimos el dataset en dos conjuntos (Train y Test) con un ratio de 0.7, hemos fijado una constante **seed** que contendrá la semilla que se va a utilizar en todo el análisis.

```{r}
seed = 1991
set.seed(seed)

# generamos los datos de TRAIN con el 70% del dataset
train.size = round(dim(datos)[1] * 0.7)
train.indexs = sample(1:dim(datos)[1], train.size)
train.data = datos[train.indexs, ]

test.indexs = -train.indexs
test.data = datos[test.indexs, ]
test.size = dim(test.data)[1]

```

### Realizamos algunas Visualizaciones
Ahora realizaremos algunas visualizaciones interesantes para entender los datos. Haremos uso de la libreria **ggplot**.

Aqui podemos ver la distribución de muestras por region, aunque no es exactamente la misma en cada region podemos ver que es bastante balanceado.
```{r}
library(ggplot2)
ggplot(data = datos) +
  geom_bar(mapping = aes(x = region))

```

Sin embargo no sucede lo mismo con la distribución de las muestras por raza, donde casi todas se concentran en un solo valor.
```{r}
ggplot(data = datos) +
  geom_bar(mapping = aes(x = race))

```
Con respecto a la variable **hispanic** sucede igual, todas las muestras se concentran en un solo valor.
```{r}
ggplot(data = datos) +
  geom_bar(mapping = aes(x = hispanic))

```


### Aplicamos un MCO a los datos 
Para tener una referencia del error de prueba al realizar predicciones en el dataset vamos a ajustar un modelo de mínimos cuadrados ordinarios (MCO) con todas las variables explicativas en el conjunto de entrenamiento, el error de prueba obtenido lo guardaremos para futuras comparaciones con métodos más avanzados.

```{r}
set.seed(seed)
mco.fit = glm(whrswk ~ ., data = train.data)
mco.pred = predict(mco.fit, newdata = test.data)

error.mco <- mean((test.data[, "whrswk"] - mco.pred) ^ 2)

summary(mco.fit)
kable(mco.fit$coefficients)
```
Aplicando el MCO obtenemos el error de prueba `r error.mco`.

### Aplicamos métodos mas avanzados

#### Usando Random Forest
A continuación usaremos el método de Random Forest en los datos de entrenamiento usaremos un valor de **ntree** que nos permita realizar las simulaciones sin consumir una cantidad excesiva de tiempo.

```{r}
library(randomForest)
library(MASS)
set.seed(seed)
Ntree = 400

rf.fit=randomForest(whrswk~.,data=train.data, ntree = Ntree)
rf.fit
rf.pred = predict(rf.fit, test.data)
error.rf = with(test.data, mean((whrswk - rf.pred)^2))

```
Como resultado obtenemos se usan `r rf.fit$mtry` (**mtry**) variables para cada partición y el error obtenido en los datos de prueba es `r error.rf`, el cual es menor que en el MCO. 

Como el número de variables independientes es `r dim(datos)[2] - 1`, calcularemos todos los `r dim(datos)[2] - 1` valores de **mtry**.

```{r}
set.seed(seed)

dimension = dim(datos)[2] - 1
oob.error = double(dimension)
error.rf.full =double(dimension)

for(mtry in 1:dimension) {
  fit = randomForest(whrswk~., data = train.data, mtry = mtry, ntree = Ntree)
  oob.error[mtry] = fit$mse[Ntree]
  pred = predict(fit, test.data)
  error.rf.full[mtry] = with(test.data, mean((whrswk - pred) ^ 2))
  cat(mtry," ")
}

matplot(1:mtry, cbind(error.rf.full, oob.error), pch=19, col=c("red", "blue"), type="b", ylab="Mean Squared Error")
legend("topright",legend=c("Test","OOB"),pch=19,col=c("red","blue"))
```

Como se puede apreciar en el gráfico el error mas pequeño es cuando se usa mtry = `r which(min(error.rf.full) == error.rf.full)`, este es de `r min(error.rf.full)`.

```{r}
# nos quedamos con el menor error de los Random Forests
error.rf = min(c(error.rf, min(error.rf.full)))
```


#### BOOSTING
Ahora usaremos BOOSTING como método mas avanzado y esperamos obtener un error aun menor.

```{r}
library(gbm)
Ntree = 1000
set.seed(seed)

cat("BOOSTING"," ")
boosting.fit = gbm(whrswk~., data=train.data, distribution = "gaussian" , n.trees = Ntree, shrinkage=0.01, interaction.depth = 4)
summary(boosting.fit)

```
Luego de entrenar el modelo en los datos de entrenamiento hacemos la predicción en los datos de prueba.
```{r}
n.trees=seq(from=100, to=Ntree, by=100)
boosting.pred = predict(boosting.fit, newdata = test.data, n.trees=n.trees)

error.boosting = with(test.data, apply((boosting.pred - whrswk) ^ 2, 2, mean))


plot(n.trees, error.boosting, pch=19, ylab="Mean Squared Error", xlab="# Trees",main="Boosting Test Error")

error.boosting = min(error.boosting)
```
El menor error de predicción usando el método de **Boosting** es `r error.boosting`.

Ahora aplicaremos validación cruzada con Boosting para intentar mejorar el resultado anterior.

```{r}
CV = 5
Ntree = 1000
set.seed(seed)

cat("BOOSTING CV"," ")
boosting.cv = gbm(whrswk~., data=train.data, distribution = "gaussian", n.trees=Ntree, cv.folds = CV, shrinkage = 0.01, interaction.depth = 4)

best_iter <- gbm.perf(boosting.cv, method="cv")
print(best_iter)

summary(boosting.cv, n.trees = Ntree)
summary(boosting.cv, n.trees = best_iter) 

n.trees = seq(from=100, to=best_iter, by=100)
predmat = predict(boosting.fit, newdata = test.data, n.trees = n.trees)

error.boosting_cv = with(test.data, apply((predmat - whrswk) ^ 2, 2, mean))
plot(n.trees, error.boosting_cv, pch=19, ylab = "Mean Squared Error", xlab = "# Trees", main = "Boosting Test Error")
error.boosting_cv = min(error.boosting_cv)
``` 

Como podemos observar en el gráfico anterior, luego de aplicar la validación cruzada con Boosting el error de prueba es `r error.boosting_cv`.

#### BOOSTING CON CARET
Ahora usaremos la libreria **caret** para hacer una búsqueda de los mejores párametros para el método de BOOSTING.
```{r}
library(caret)
set.seed(seed)
Ntree = 1000

gbmGrid <- expand.grid(interaction.depth = 4,
                       n.trees = seq(200, Ntree, 200),
                       shrinkage = c(0.01, 0.05),
                       n.minobsinnode = 10)
fitControl <- trainControl(method = 'cv', number = 5, summaryFunction = defaultSummary)

gradientBoosting.fit <- train(whrswk~., data = train.data, method = 'gbm', trControl=fitControl, tuneGrid=gbmGrid, metric='RMSE')

plot(gradientBoosting.fit)

```
Después de realizar la búsqueda de los mejores parámetros nos queda que son `r gradientBoosting.fit$bestTune`.

```{r}
res <- gradientBoosting.fit$results
RMSE <- subset(res[5])
```
El error de entrenamiento nos da como resultado `r RMSE`.

Ahora procedemos a utilizar el modelo para realizar la predicción el los datos de prueba.
```{r}
gradientBoosting.pred <- predict(gradientBoosting.fit, test.data)
error.gradientBoosting = (mean((gradientBoosting.pred - test.data$whrswk) ^ 2))
```

El error en los datos de prueba nos queda `r error.gradientBoosting`.

### Comparamos y gráficamos los resultados
```{r}
```
